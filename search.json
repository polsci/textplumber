[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "textplumber",
    "section": "",
    "text": "This package is currently for testing only.",
    "crumbs": [
      "textplumber"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "textplumber",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/polsci/textplumber.git\nor from pypi\n$ pip install textplumber\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on pypi.",
    "crumbs": [
      "textplumber"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "textplumber",
    "section": "How to use",
    "text": "How to use\nTHe intention of textplumber is to make it easy to extract features from text data for use in Sci-kit learn pipelines. It allows you to combine different kinds of features extracted from text. Here’s an example …\nFirst, let’s load some data …\n\nfrom sklearn.datasets import fetch_20newsgroups\nimport pandas as pd\n\ncats = ['talk.politics.misc', 'talk.religion.misc']\ndataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=cats)\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size = 0.2, random_state=42)\n\ntarget_names = dataset.target_names\ntarget_classes = list(range(len(target_names)))\n\nNext, define an SQLite database to store features so we can avoid recomputing them every time we run the pipeline. A feature store is required for most of the components in this package.\n\nfeature_store = TextFeatureStore('feature_store.sqlite')\n\nNow, setup a pipeline …\n\npipeline = Pipeline([\n    ('cleaner', TextCleaner(strip_whitespace=True)),\n    ('preprocessor', SpacyPreprocessor(feature_store=feature_store)),\n    ('tokens', TokensVectorizer(feature_store=feature_store, lowercase=True, max_features=1000, remove_punctuation = True, remove_numbers=True, min_token_length=2)),\n    ('classifier', MultinomialNB()),\n], verbose=True)\n\ndisplay(pipeline)\n\nPipeline(steps=[('cleaner', TextCleaner(strip_whitespace=True)),\n                ('preprocessor',\n                 SpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;)),\n                ('tokens',\n                 TokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                                  lowercase=True, max_features=1000,\n                                  min_token_length=2, remove_numbers=True,\n                                  remove_punctuation=True)),\n                ('classifier', MultinomialNB())],\n         verbose=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('cleaner', TextCleaner(strip_whitespace=True)),\n                ('preprocessor',\n                 SpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;)),\n                ('tokens',\n                 TokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                                  lowercase=True, max_features=1000,\n                                  min_token_length=2, remove_numbers=True,\n                                  remove_punctuation=True)),\n                ('classifier', MultinomialNB())],\n         verbose=True) TextCleanerTextCleaner(strip_whitespace=True) SpacyPreprocessorSpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;) TokensVectorizerTokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                 lowercase=True, max_features=1000, min_token_length=2,\n                 remove_numbers=True, remove_punctuation=True) MultinomialNB?Documentation for MultinomialNBMultinomialNB() \n\n\nIn the example pipeline, the texts are cleaned with TextCleaner, then preprocessed using SpacyPreprocessor, and then vectors based on token counts are extracted using TokensVectorizer.\nNow, let’s train a model …\n\npipeline.fit(X_train, y_train)\n\n[Pipeline] ........... (step 1 of 4) Processing cleaner, total=   0.0s\n[Pipeline] ...... (step 2 of 4) Processing preprocessor, total=  15.4s\n[Pipeline] ............ (step 3 of 4) Processing tokens, total=   0.1s\n[Pipeline] ........ (step 4 of 4) Processing classifier, total=   0.0s\n\n\nPipeline(steps=[('cleaner', TextCleaner(strip_whitespace=True)),\n                ('preprocessor',\n                 SpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;)),\n                ('tokens',\n                 TokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                                  lowercase=True, max_features=1000,\n                                  min_token_length=2, remove_numbers=True,\n                                  remove_punctuation=True)),\n                ('classifier', MultinomialNB())],\n         verbose=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('cleaner', TextCleaner(strip_whitespace=True)),\n                ('preprocessor',\n                 SpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;)),\n                ('tokens',\n                 TokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                                  lowercase=True, max_features=1000,\n                                  min_token_length=2, remove_numbers=True,\n                                  remove_punctuation=True)),\n                ('classifier', MultinomialNB())],\n         verbose=True) TextCleanerTextCleaner(strip_whitespace=True) SpacyPreprocessorSpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;) TokensVectorizerTokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                 lowercase=True, max_features=1000, min_token_length=2,\n                 remove_numbers=True, remove_punctuation=True) MultinomialNB?Documentation for MultinomialNBMultinomialNB() \n\n\nWe can evaluate the model on the test set. To output a nice confusion matrix use plot_confusion_matrix.\n\ny_pred = pipeline.predict(X_test)\nprint(classification_report(y_test, y_pred, labels = target_classes, target_names = target_names, digits=3))\nplot_confusion_matrix(y_test, y_pred, target_classes, target_names)\n\n                    precision    recall  f1-score   support\n\ntalk.politics.misc      0.816     0.772     0.793        92\ntalk.religion.misc      0.744     0.792     0.767        77\n\n          accuracy                          0.781       169\n         macro avg      0.780     0.782     0.780       169\n      weighted avg      0.783     0.781     0.781       169\n\n\n\n\n\n\n\n\n\n\nLet’s create a more complex pipeline with different kinds of features, but first we need our lexicon.\n\nempath_lexicons = get_empath_lexicons()\n\nNow, let’s build a pipeline that combines tf-idf weighted token features and embeddings. This applies feature selection using standard scikit-learn components.\n\npipeline = Pipeline([\n    ('cleaner', TextCleaner(strip_whitespace=True)),\n    ('preprocessor', SpacyPreprocessor(feature_store=feature_store)),\n    ('features', FeatureUnion([\n\n        ('tokens', Pipeline([\n            ('tokens_vectors',  TokensVectorizer(feature_store=feature_store, vectorizer_type = 'tfidf', lowercase=True, max_features=5000, remove_punctuation = True, remove_numbers=True, min_token_length=2)),\n            ('selector', SelectKBest(score_func=chi2, k=100)),\n            ], verbose=True)\n        ),\n        ('embeddings', Model2VecEmbedder(feature_store=feature_store)),\n    ], verbose=True)),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=4)),\n], verbose=True)\n\ndisplay(pipeline)\n\nPipeline(steps=[('cleaner', TextCleaner(strip_whitespace=True)),\n                ('preprocessor',\n                 SpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;)),\n                ('features',\n                 FeatureUnion(transformer_list=[('tokens',\n                                                 Pipeline(steps=[('tokens_vectors',\n                                                                  TokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                                                                                   l...\n                                                                                   remove_numbers=True,\n                                                                                   remove_punctuation=True,\n                                                                                   vectorizer_type='tfidf')),\n                                                                 ('selector',\n                                                                  SelectKBest(k=100,\n                                                                              score_func=&lt;function chi2 at 0x7f875e8c93a0&gt;))],\n                                                          verbose=True)),\n                                                ('embeddings',\n                                                 Model2VecEmbedder(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;))],\n                              verbose=True)),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, random_state=4))],\n         verbose=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('cleaner', TextCleaner(strip_whitespace=True)),\n                ('preprocessor',\n                 SpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;)),\n                ('features',\n                 FeatureUnion(transformer_list=[('tokens',\n                                                 Pipeline(steps=[('tokens_vectors',\n                                                                  TokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                                                                                   l...\n                                                                                   remove_numbers=True,\n                                                                                   remove_punctuation=True,\n                                                                                   vectorizer_type='tfidf')),\n                                                                 ('selector',\n                                                                  SelectKBest(k=100,\n                                                                              score_func=&lt;function chi2 at 0x7f875e8c93a0&gt;))],\n                                                          verbose=True)),\n                                                ('embeddings',\n                                                 Model2VecEmbedder(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;))],\n                              verbose=True)),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, random_state=4))],\n         verbose=True) TextCleanerTextCleaner(strip_whitespace=True) SpacyPreprocessorSpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;) features: FeatureUnion?Documentation for features: FeatureUnionFeatureUnion(transformer_list=[('tokens',\n                                Pipeline(steps=[('tokens_vectors',\n                                                 TokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                                                                  lowercase=True,\n                                                                  min_token_length=2,\n                                                                  remove_numbers=True,\n                                                                  remove_punctuation=True,\n                                                                  vectorizer_type='tfidf')),\n                                                ('selector',\n                                                 SelectKBest(k=100,\n                                                             score_func=&lt;function chi2 at 0x7f875e8c93a0&gt;))],\n                                         verbose=True)),\n                               ('embeddings',\n                                Model2VecEmbedder(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;))],\n             verbose=True) tokensTokensVectorizerTokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                 lowercase=True, min_token_length=2, remove_numbers=True,\n                 remove_punctuation=True, vectorizer_type='tfidf') SelectKBest?Documentation for SelectKBestSelectKBest(k=100, score_func=&lt;function chi2 at 0x7f875e8c93a0&gt;) embeddingsModel2VecEmbedderModel2VecEmbedder(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;) LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=4) \n\n\n\npipeline.fit(X_train, y_train)\n\n[Pipeline] ........... (step 1 of 4) Processing cleaner, total=   0.0s\n[Pipeline] ...... (step 2 of 4) Processing preprocessor, total=   0.0s\n[Pipeline] .... (step 1 of 2) Processing tokens_vectors, total=   0.1s\n[Pipeline] .......... (step 2 of 2) Processing selector, total=   0.0s\n[FeatureUnion] ........ (step 1 of 2) Processing tokens, total=   0.1s\n[FeatureUnion] .... (step 2 of 2) Processing embeddings, total=   5.8s\n[Pipeline] .......... (step 3 of 4) Processing features, total=   5.9s\n[Pipeline] ........ (step 4 of 4) Processing classifier, total=   0.0s\n\n\nPipeline(steps=[('cleaner', TextCleaner(strip_whitespace=True)),\n                ('preprocessor',\n                 SpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;)),\n                ('features',\n                 FeatureUnion(transformer_list=[('tokens',\n                                                 Pipeline(steps=[('tokens_vectors',\n                                                                  TokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                                                                                   l...\n                                                                                   remove_numbers=True,\n                                                                                   remove_punctuation=True,\n                                                                                   vectorizer_type='tfidf')),\n                                                                 ('selector',\n                                                                  SelectKBest(k=100,\n                                                                              score_func=&lt;function chi2 at 0x7f875e8c93a0&gt;))],\n                                                          verbose=True)),\n                                                ('embeddings',\n                                                 Model2VecEmbedder(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;))],\n                              verbose=True)),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, random_state=4))],\n         verbose=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('cleaner', TextCleaner(strip_whitespace=True)),\n                ('preprocessor',\n                 SpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;)),\n                ('features',\n                 FeatureUnion(transformer_list=[('tokens',\n                                                 Pipeline(steps=[('tokens_vectors',\n                                                                  TokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                                                                                   l...\n                                                                                   remove_numbers=True,\n                                                                                   remove_punctuation=True,\n                                                                                   vectorizer_type='tfidf')),\n                                                                 ('selector',\n                                                                  SelectKBest(k=100,\n                                                                              score_func=&lt;function chi2 at 0x7f875e8c93a0&gt;))],\n                                                          verbose=True)),\n                                                ('embeddings',\n                                                 Model2VecEmbedder(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;))],\n                              verbose=True)),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, random_state=4))],\n         verbose=True) TextCleanerTextCleaner(strip_whitespace=True) SpacyPreprocessorSpacyPreprocessor(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;) features: FeatureUnion?Documentation for features: FeatureUnionFeatureUnion(transformer_list=[('tokens',\n                                Pipeline(steps=[('tokens_vectors',\n                                                 TokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                                                                  lowercase=True,\n                                                                  min_token_length=2,\n                                                                  remove_numbers=True,\n                                                                  remove_punctuation=True,\n                                                                  vectorizer_type='tfidf')),\n                                                ('selector',\n                                                 SelectKBest(k=100,\n                                                             score_func=&lt;function chi2 at 0x7f875e8c93a0&gt;))],\n                                         verbose=True)),\n                               ('embeddings',\n                                Model2VecEmbedder(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;))],\n             verbose=True) tokensTokensVectorizerTokensVectorizer(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;,\n                 lowercase=True, min_token_length=2, remove_numbers=True,\n                 remove_punctuation=True, vectorizer_type='tfidf') SelectKBest?Documentation for SelectKBestSelectKBest(k=100, score_func=&lt;function chi2 at 0x7f875e8c93a0&gt;) embeddingsModel2VecEmbedderModel2VecEmbedder(feature_store=&lt;textplumber.store.TextFeatureStore object at 0x7f8752906a10&gt;) LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, random_state=4) \n\n\n\ny_pred = pipeline.predict(X_test)\nprint(classification_report(y_test, y_pred, labels = target_classes, target_names = target_names, digits=3))\nplot_confusion_matrix(y_test, y_pred, target_classes, target_names)\n\n                    precision    recall  f1-score   support\n\ntalk.politics.misc      0.830     0.902     0.865        92\ntalk.religion.misc      0.870     0.779     0.822        77\n\n          accuracy                          0.846       169\n         macro avg      0.850     0.841     0.843       169\n      weighted avg      0.848     0.846     0.845       169\n\n\n\n\n\n\n\n\n\n\n\n# ('lexicon', Pipeline([\n        #     ('lexicon_vectors', LexiconCountVectorizer(feature_store=feature_store, lexicons=empath_lexicons)),\n        #     ('scaler', StandardScaler(with_mean=False))\n        #     ], verbose=True)\n        # ),\n        # ('pos', Pipeline([\n        #     ('pos_vectors',  POSVectorizer(feature_store=feature_store)),\n        #     ('selector', StandardScaler(with_mean=False)),\n        #     ], verbose=True)\n        # ),\n        # ('textstats', Pipeline([\n        #     ('textstats_vectors', TextstatsTransformer(feature_store=feature_store)),\n        #     ('scaler', StandardScaler(with_mean=False))\n        #     ], verbose=True)\n        # ),\n\n\n# #| hide\nos.remove('feature_store.sqlite')",
    "crumbs": [
      "textplumber"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "textplumber",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall textplumber in Development mode\n# make sure textplumber package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to textplumber\n$ nbdev_prepare",
    "crumbs": [
      "textplumber"
    ]
  },
  {
    "objectID": "embeddings.html",
    "href": "embeddings.html",
    "title": "embeddings",
    "section": "",
    "text": "source\n\nModel2VecEmbedder\n\n Model2VecEmbedder (feature_store:textplumber.store.TextFeatureStore,\n                    model_name:str='minishlab/potion-base-8M')\n\nSci-kit Learn pipeline component to generate embeddings using Model2Vec.\nTODO: add an example.",
    "crumbs": [
      "embeddings"
    ]
  },
  {
    "objectID": "pos.html",
    "href": "pos.html",
    "title": "pos",
    "section": "",
    "text": "source\n\nPOSVectorizer\n\n POSVectorizer (feature_store:textplumber.store.TextFeatureStore,\n                ngram_range:tuple=(1, 1), vocabulary:list|None=None)\n\nSci-kit Learn pipeline component to extract parts of speech tag features. This component should be used after the SpacyPreprocessor component with the same feature store. The component gets the tokens from the feature store and returns a matrix of counts (via CountVectorizer).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfeature_store\nTextFeatureStore\n\nthe feature store to use - this should be the same feature store used in the SpacyPreprocessor component\n\n\nngram_range\ntuple\n(1, 1)\nthe ngram range to use (min_n, max_n) - passed to CountVectorizer\n\n\nvocabulary\nlist | None\nNone\nlist of tokens to use - passed to CountVectorizer\n\n\n\nTODO: add an example.",
    "crumbs": [
      "pos"
    ]
  },
  {
    "objectID": "store.html",
    "href": "store.html",
    "title": "store",
    "section": "",
    "text": "source\n\nTextFeatureStore\n\n TextFeatureStore (path:str)\n\nA class to store features extracted for a text classification pipeline and cache them to disk to avoid recomputing them.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\nwhere to save sqllite db to persist the feature store between runs\n\n\n\nTODO: add an example.",
    "crumbs": [
      "store"
    ]
  },
  {
    "objectID": "lexicons.html",
    "href": "lexicons.html",
    "title": "lexicons",
    "section": "",
    "text": "source\n\nLexiconCountVectorizer\n\n LexiconCountVectorizer (feature_store:textplumber.store.TextFeatureStore,\n                         lexicons:dict, lowercase:bool=True)\n\nA Sci-kit Learn pipeline component to get document-level counts for one or more lexicons. This component should be used after the SpacyPreprocessor component with the same feature store.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfeature_store\nTextFeatureStore\n\nthe feature store to use - this should be the same feature store used in the SpacyPreprocessor component\n\n\nlexicons\ndict\n\nthe lexicons to use - a dictionary with the lexicon name as the key and the lexicon (a list of tokens to count) as the value\n\n\nlowercase\nbool\nTrue\nwhether to lowercase the tokens\n\n\n\n\nsource\n\n\nget_empath_lexicons\n\n get_empath_lexicons ()\n\nGet the empath lexicons from the empath github repo.\nTODO: add an example.",
    "crumbs": [
      "lexicons"
    ]
  },
  {
    "objectID": "tokens.html",
    "href": "tokens.html",
    "title": "tokens",
    "section": "",
    "text": "source\n\nTokensVectorizer\n\n TokensVectorizer (feature_store:textplumber.store.TextFeatureStore,\n                   vectorizer_type:str='count', lowercase:bool=False,\n                   min_token_length:int=0, remove_punctuation:bool=False,\n                   remove_numbers:bool=False,\n                   stop_words:list[str]|None=None, min_df:float|int=1,\n                   max_df:float|int=1.0, max_features:int=5000,\n                   ngram_range:tuple=(1, 1), vocabulary:list|None=None,\n                   encoding:str='utf-8', decode_error:str='ignore')\n\nSci-kit Learn pipeline component to extract token features. This component should be used after the SpacyPreprocessor component with the same feature store. The component gets the tokens from the feature store and returns a matrix of counts (via CountVectorizer) or Tf-idf scores (using TfidfVectorizer).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfeature_store\nTextFeatureStore\n\nthe feature store to use - this should be the same feature store used in the SpacyPreprocessor component\n\n\nvectorizer_type\nstr\ncount\nthe type of vectorizer to use - ‘count’ for CountVectorizer or ‘tfidf’ for TfidfVectorizer\n\n\nlowercase\nbool\nFalse\nwhether to lowercase the tokens\n\n\nmin_token_length\nint\n0\nthe minimum token length to use\n\n\nremove_punctuation\nbool\nFalse\nwhether to remove punctuation from the tokens\n\n\nremove_numbers\nbool\nFalse\nwhether to remove numbers from the tokens\n\n\nstop_words\nlist[str] | None\nNone\nthe stop words to use - passed to CountVectorizer or TfidfVectorizer\n\n\nmin_df\nfloat | int\n1\nthe minimum document frequency to use - passed to CountVectorizer or TfidfVectorizer\n\n\nmax_df\nfloat | int\n1.0\nthe maximum document frequency to use - passed to CountVectorizer or TfidfVectorizer\n\n\nmax_features\nint\n5000\nthe maximum number of features to use, setting a default to avoid memory issues - passed to CountVectorizer or TfidfVectorizer\n\n\nngram_range\ntuple\n(1, 1)\nthe ngram range to use (min_n, max_n) - passed to CountVectorizer or TfidfVectorizer\n\n\nvocabulary\nlist | None\nNone\nlist of tokens to use - passed to CountVectorizer or TfidfVectorizer\n\n\nencoding\nstr\nutf-8\nthe encoding to use - passed to CountVectorizer or TfidfVectorizer\n\n\ndecode_error\nstr\nignore\nwhat to do if there is an error decoding ‘strict’, ‘ignore’, ‘replace’ - passed to CountVectorizer or TfidfVectorizer\n\n\n\nTODO: add an example.",
    "crumbs": [
      "tokens"
    ]
  },
  {
    "objectID": "clean.html",
    "href": "clean.html",
    "title": "cleaner",
    "section": "",
    "text": "source\n\nTextCleaner\n\n TextCleaner (character_replacements:dict=None, remove_html:bool=False,\n              strip_whitespace:bool=False,\n              normalize_whitespace:bool=False)\n\nA component for a Sci-kit learn pipeline to clean clean text data, including normalizing characters and whitespace, stripping whitespace before and after text, and removing html tags, .\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncharacter_replacements\ndict\nNone\ncharacter replacements for character normalization - a dict with key as the character to replace and value as the replacement character\n\n\nremove_html\nbool\nFalse\nwhether to remove html tags\n\n\nstrip_whitespace\nbool\nFalse\nwhether to remove whitespace from the start and end of the text\n\n\nnormalize_whitespace\nbool\nFalse\nwhether to replace one or more whitespace characters with a single space\n\n\n\nHere’s an example that removes html, strips and normalizes whitespace and normalizes single quotes.\nOnly apply what makes sense for your use case.\nNote: The character_replacement dictionary is used to specify single-character replacements. This will raise a ValueError if the input strings are longer than 1 character.\n\ndocuments = [\n    \"&lt;p&gt;Some text with &lt;b&gt;html&lt;/b&gt; tags&lt;/p&gt;\",\n    \"Some text with      extra whitespace\",\n    \"Some text with ‘single quotes’\",\n    \"Some text with  \\t a tab character\",\n    \"   Some text with whitespace before and after the text  \\n \",\n]\n\ncharacter_replacements ={\n    \"‘\": \"'\",\n    \"’\": \"'\",\n}\n\ntext_cleaner = TextCleaner(\n    character_replacements=character_replacements,\n    remove_html=True,\n    strip_whitespace=True,\n    normalize_whitespace=True,\n)\ncleaned_documents = text_cleaner.fit_transform(documents)\n\nfor i, doc in enumerate(cleaned_documents):\n    print(f\"Original: {documents[i]}\")\n    print(f\"Cleaned: {doc}\")\n    print()\n\nOriginal: &lt;p&gt;Some text with &lt;b&gt;html&lt;/b&gt; tags&lt;/p&gt;\nCleaned: Some text with html tags\n\nOriginal: Some text with      extra whitespace\nCleaned: Some text with extra whitespace\n\nOriginal: Some text with ‘single quotes’\nCleaned: Some text with 'single quotes'\n\nOriginal: Some text with     a tab character\nCleaned: Some text with a tab character\n\nOriginal:    Some text with whitespace before and after the text  \n \nCleaned: Some text with whitespace before and after the text",
    "crumbs": [
      "cleaner"
    ]
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "report",
    "section": "",
    "text": "source\n\npreview_splits\n\n preview_splits (X_train, y_train, X_test, y_test, label_names)\n\n\nsource\n\n\npreview_label_counts\n\n preview_label_counts (df, label_column, label_names)\n\n\nsource\n\n\npreview_text_field\n\n preview_text_field (text:str, width:int=80)\n\nPreview a text field, wrapping the text to 80 characters\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntext\nstr\n\nText to preview\n\n\nwidth\nint\n80\nWidth to wrap the text to\n\n\n\n\nsource\n\n\npreview_row_text\n\n preview_row_text (df:pandas.core.frame.DataFrame, selected_index:int,\n                   text_column:str='text', limit:int=-1)\n\nPreview the text fields of a row in the DataFrame\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nDataFrame containing the data\n\n\nselected_index\nint\n\nIndex of the row to preview\n\n\ntext_column\nstr\ntext\ncolumn name for text field\n\n\nlimit\nint\n-1\nLimit the length of the text field\n\n\n\n\nsource\n\n\nplot_confusion_matrix\n\n plot_confusion_matrix (y_test, y_predicted, target_classes, target_names)",
    "crumbs": [
      "report"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\npass_tokens\n\n pass_tokens (tokens)\n\nPass through function to pass pre-tokenized input to CountVectorizer or TfidfVectorizer.\n\nsource\n\n\nget_stop_words\n\n get_stop_words ()\n\nGet stop words from NLTK.",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "preprocess.html",
    "href": "preprocess.html",
    "title": "preprocess",
    "section": "",
    "text": "source\n\nSpacyPreprocessor\n\n SpacyPreprocessor (feature_store:textplumber.store.TextFeatureStore,\n                    model_name:str='en_core_web_sm',\n                    disable:list[str]=['parser', 'ner'],\n                    enable:list[str]=['sentencizer'], batch_size:int=500,\n                    n_process:int=1)\n\nA Sci-kit Learn pipeline component to preprocess text using spaCy, the pipeline component receives and returns texts, but prepares tokens, pos, and text statistics as input to other compatible classes in a pipeline.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfeature_store\nTextFeatureStore\n\nthe feature store to use\n\n\nmodel_name\nstr\nen_core_web_sm\nthe spaCy model to use\n\n\ndisable\nlist\n[‘parser’, ‘ner’]\nthe spaCy components to disable\n\n\nenable\nlist\n[‘sentencizer’]\nthe spaCy components to enable\n\n\nbatch_size\nint\n500\nthe batch size to use\n\n\nn_process\nint\n1\nthe number of processes to use\n\n\n\nTODO: add an example.",
    "crumbs": [
      "preprocess"
    ]
  },
  {
    "objectID": "textstats.html",
    "href": "textstats.html",
    "title": "textstats",
    "section": "",
    "text": "source\n\nTextstatsTransformer\n\n TextstatsTransformer (feature_store:textplumber.store.TextFeatureStore,\n                       columns=['monosyll_count', 'polysyll_count',\n                       'token_count', 'sentence_count',\n                       'unique_tokens_count', 'average_sentence_length'])\n\nSci-kit Learn pipeline component to extract document-level text statistics based on the textstat library and pre-computed counts. This component should be used after the SpacyPreprocessor component with the same feature store. The statistics currently available are monosyllable count, polysyllable count, token count, sentence count, unique tokens count and average sentence length.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfeature_store\nTextFeatureStore\n\nthe feature store to use\n\n\ncolumns\nlist\n[‘monosyll_count’, ‘polysyll_count’, ‘token_count’, ‘sentence_count’, ‘unique_tokens_count’, ‘average_sentence_length’]\n\n\n\n\nTODO: add an example.",
    "crumbs": [
      "textstats"
    ]
  }
]