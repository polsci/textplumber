{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokens\n",
    "\n",
    "> Extract token features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from textplumber.store import TextFeatureStore\n",
    "from textplumber.core import pass_tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TokensVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Sci-kit Learn pipeline component to extract token features. This component should be used after the SpacyPreprocessor component with the same feature store.\n",
    "        The component gets the tokens from the feature store and returns a matrix of counts (via CountVectorizer) or Tf-idf scores (using TfidfVectorizer). \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 feature_store: TextFeatureStore, # the feature store to use - this should be the same feature store used in the SpacyPreprocessor component\n",
    "                 vectorizer_type:str = 'count', # the type of vectorizer to use - 'count' for CountVectorizer or 'tfidf' for TfidfVectorizer\n",
    "                 lowercase:bool = False, # whether to lowercase the tokens \n",
    "                 min_token_length:int = 0, # the minimum token length to use\n",
    "                 remove_punctuation:bool = False, # whether to remove punctuation from the tokens\n",
    "                 remove_numbers:bool = False, # whether to remove numbers from the tokens\n",
    "                 stop_words:list[str]|None = None, # the stop words to use - passed to CountVectorizer or TfidfVectorizer\n",
    "                 min_df:float|int = 1, # the minimum document frequency to use - passed to CountVectorizer or TfidfVectorizer\n",
    "                 max_df:float|int = 1.0, # the maximum document frequency to use - passed to CountVectorizer or TfidfVectorizer\n",
    "                 max_features:int = 5000, # the maximum number of features to use, setting a default to avoid memory issues - passed to CountVectorizer or TfidfVectorizer\n",
    "                 ngram_range:tuple = (1, 1), # the ngram range to use (min_n, max_n) - passed to CountVectorizer or TfidfVectorizer\n",
    "                 vocabulary:list|None = None, # list of tokens to use - passed to CountVectorizer or TfidfVectorizer\n",
    "                 encoding:str = 'utf-8', # the encoding to use - passed to CountVectorizer or TfidfVectorizer \n",
    "                 decode_error:str = 'ignore' # what to do if there is an error decoding 'strict', 'ignore', 'replace' - passed to CountVectorizer or TfidfVectorizer\n",
    "                ):\n",
    "        self.vectorizer_type = vectorizer_type\n",
    "        self.feature_store = feature_store\n",
    "        self.lowercase = lowercase\n",
    "        self.min_token_length = min_token_length\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.stop_words = stop_words\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range = ngram_range\n",
    "        self.vocabulary = vocabulary\n",
    "        self.encoding = encoding\n",
    "        self.decode_error = decode_error\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Fit the vectorizer to the tokens in the feature store. \"\"\"\n",
    "        if self.vectorizer_type == 'tfidf':\n",
    "            self.vectorizer_ = TfidfVectorizer(tokenizer=pass_tokens, lowercase=False, token_pattern = None, stop_words=self.stop_words, min_df=self.min_df, max_df=self.max_df, max_features=self.max_features, ngram_range=self.ngram_range, vocabulary= self.vocabulary, encoding=self.encoding, decode_error=self.decode_error)\n",
    "        elif self.vectorizer_type == 'count':\n",
    "            self.vectorizer_ = CountVectorizer(tokenizer=pass_tokens, lowercase=False, token_pattern = None, stop_words=self.stop_words, min_df=self.min_df, max_df=self.max_df, max_features=self.max_features, ngram_range=self.ngram_range, vocabulary= self.vocabulary, encoding=self.encoding, decode_error=self.decode_error)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid vectorizer_type. Use 'tfidf' or 'count'.\")\n",
    "        tokens = self.feature_store.get_tokens_from_texts(X, lowercase = self.lowercase, min_token_length = self.min_token_length, remove_punctuation = self.remove_punctuation, remove_numbers = self.remove_numbers)\n",
    "        self.vectorizer_.fit(tokens, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\" Transform the texts to a matrix of counts or tf-idf scores. \"\"\"\n",
    "        tokens = self.feature_store.get_tokens_from_texts(X, lowercase = self.lowercase, min_token_length = self.min_token_length, remove_punctuation = self.remove_punctuation, remove_numbers = self.remove_numbers)\n",
    "        return self.vectorizer_.transform(tokens)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\" Get the feature names out from the vectorizer. \"\"\"\n",
    "        return self.vectorizer_.get_feature_names_out(input_features)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "from textplumber.preprocess import SpacyPreprocessor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "feature_store = TextFeatureStore('test_tokens.sqlite')\n",
    "spacy_preprocessor = SpacyPreprocessor(feature_store=feature_store)\n",
    "spacy_tokens_vectorizer = TokensVectorizer(feature_store=feature_store, vectorizer_type='count', lowercase=False)\n",
    "pipeline = Pipeline([\n",
    "    ('spacy_preprocessor', spacy_preprocessor),\n",
    "    ('spacy_tokens_vectorizer', spacy_tokens_vectorizer)\n",
    "])\n",
    "pipeline.fit(['Hello, world!'])\n",
    "X = pipeline.transform(['Hello, world!'])\n",
    "assert tuple(X.toarray()[0]) == tuple([1, 1, 1, 1])\n",
    "\n",
    "spacy_tokens_vectorizer = TokensVectorizer(feature_store=feature_store, vectorizer_type='count', lowercase=True)\n",
    "pipeline = Pipeline([\n",
    "    ('spacy_preprocessor', spacy_preprocessor),\n",
    "    ('spacy_tokens_vectorizer', spacy_tokens_vectorizer)\n",
    "])\n",
    "pipeline.fit(['Hello, world!', 'hello, world!'])\n",
    "X = pipeline.transform(['Hello, world!', 'hello, world!'])\n",
    "# docs should be equivalent\n",
    "assert tuple(X.toarray()[1]) == tuple(X.toarray()[1])\n",
    "\n",
    "# test vocabulary\n",
    "spacy_tokens_vectorizer = TokensVectorizer(feature_store=feature_store, vectorizer_type='count', lowercase=True, vocabulary=['hello', 'world'])\n",
    "pipeline = Pipeline([\n",
    "    ('spacy_preprocessor', spacy_preprocessor),\n",
    "    ('spacy_tokens_vectorizer', spacy_tokens_vectorizer)\n",
    "])\n",
    "pipeline.fit(['Hello, world!', 'hello, world!'])\n",
    "X = pipeline.transform(['Hello, world!', 'hello, world!'])\n",
    "assert tuple(X.toarray()[0]) == tuple([1, 1])\n",
    "assert tuple(X.toarray()[1]) == tuple([1, 1])\n",
    "id = spacy_tokens_vectorizer.get_feature_names_out().tolist().index('hello')\n",
    "assert X.todense()[0, id] == 1\n",
    "assert X.todense()[1, id] == 1\n",
    "X = pipeline.transform(['something else'])\n",
    "assert tuple(X.toarray()[0]) == tuple([0, 0])\n",
    "\n",
    "# test empty text\n",
    "spacy_tokens_vectorizer = TokensVectorizer(feature_store=feature_store, vectorizer_type='count', lowercase=True)\n",
    "pipeline = Pipeline([\n",
    "    ('spacy_preprocessor', spacy_preprocessor),\n",
    "    ('spacy_tokens_vectorizer', spacy_tokens_vectorizer)\n",
    "])\n",
    "pipeline.fit(['Hello, world!', 'hello, world!'])\n",
    "X = pipeline.transform(['Hello, world!', 'hello, world!', ''])\n",
    "assert tuple(X.toarray()[0]) == tuple([1, 1, 1, 1])\n",
    "assert tuple(X.toarray()[1]) == tuple([1, 1, 1, 1])\n",
    "assert tuple(X.toarray()[2]) == tuple([0, 0, 0, 0])\n",
    "\n",
    "os.remove('test_tokens.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
