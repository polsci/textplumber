{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings\n",
    "\n",
    "> Extract text embedding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from textplumber.store import TextFeatureStore\n",
    "from model2vec import StaticModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Model2VecEmbedder(BaseEstimator, TransformerMixin):\n",
    "\t\"\"\" Sci-kit Learn pipeline component to generate embeddings using Model2Vec. \"\"\"\n",
    "\tdef __init__(self, \n",
    "                feature_store: TextFeatureStore, # the feature store to use - this should be the same feature store used in the SpacyPreprocessor component\n",
    "\t\t\t\tmodel_name:str = 'minishlab/potion-base-8M' # the model name to use\n",
    "\t\t\t\t):\n",
    "\t\tself.feature_store = feature_store\n",
    "\t\tself.model_name = model_name\n",
    "\t\tself.model_ = StaticModel.from_pretrained(self.model_name)\n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\t\"\"\" Fit is implemented but does nothing. \"\"\"\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\t\"\"\" Generate embeddings for the texts using the Model2Vec model. \n",
    "\t\tIf the embeddings are already in the feature store, they are used instead of recomputing them. Processing is done in batches of \n",
    "\t\t1000 texts to avoid memory issues. \"\"\"\n",
    "\t\tembeddings = self.feature_store.get_embeddings_from_texts(X)\n",
    "\t\tif any(x is None for x in embeddings):\n",
    "\t\t\tembeddings = []\n",
    "\t\t\tfor i in range(0, len(X), 1000):\n",
    "\t\t\t\tX_batch = X[i:i+1000]\n",
    "\t\t\t\tembeddings_batch = self.model_.encode(X_batch)\n",
    "\t\t\t\tembeddings_batch = np.array(embeddings_batch, dtype=np.double) # returning as floats seemed to be causing issues with kmeans pipeline component\n",
    "\t\t\t\tembeddings.append(embeddings_batch)\n",
    "\t\t\tembeddings = np.concatenate(embeddings, axis=0)\n",
    "\t\t\tself.feature_store.update_embeddings(X, embeddings)\n",
    "\t\telse:\n",
    "\t\t\t# all the embeddings are already in the feature store so no need to reprocess\n",
    "\t\t\tpass\n",
    "\t\treturn embeddings\n",
    "\t\n",
    "\tdef get_feature_names_out(self):\n",
    "\t\t\"\"\" Get the feature names out from the model. \"\"\"\n",
    "\t\treturn [f'emb_{i}' for i in range(self.model_.dim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "feature_store = TextFeatureStore('test_embeddings.sqlite')\n",
    "model2vec_embedder = Model2VecEmbedder(feature_store=feature_store)\n",
    "model2vec_embedder.fit(['Hello, world!'])\n",
    "X = model2vec_embedder.fit_transform(['Hello, world!'])\n",
    "assert X.shape == (1, 256)\n",
    "del feature_store\n",
    "os.remove('test_embeddings.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
