{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess\n",
    "\n",
    "> Preprocess text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import spacy\n",
    "from textplumber.store import TextFeatureStore\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\t\"\"\" A Sci-kit Learn pipeline component to preprocess text using spaCy, \n",
    "\t\tthe pipeline component receives and returns texts, but prepares tokens, pos, and text statistics \n",
    "\t\tas input to other compatible classes in a pipeline. \"\"\"\n",
    "\tdef __init__(self, \n",
    "\t\t\t\tfeature_store: TextFeatureStore, # the feature store to use\n",
    "\t\t\t\tmodel_name:str = 'en_core_web_sm', # the spaCy model to use\n",
    "\t\t\t\tdisable: list[str] = ['parser', 'ner'], # the spaCy components to disable\n",
    "\t\t\t\tenable: list[str] = ['sentencizer'], # the spaCy components to enable\n",
    "\t\t\t\tbatch_size:int = 500, # the batch size to use\n",
    "\t\t\t\tn_process:int = 1 # the number of processes to use\n",
    "\t\t\t\t ):\n",
    "\t\tself.model_name = model_name\n",
    "\t\tself.disable = disable\n",
    "\t\tself.enable = enable\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.n_process = n_process\n",
    "\t\tself.nlp = spacy.load(self.model_name, disable=self.disable)\n",
    "\t\tfor component in self.enable:\n",
    "\t\t\tself.nlp.add_pipe(component)\n",
    "\n",
    "\t\tself.feature_store = feature_store\n",
    "\n",
    "\tdef _iterator(self, \n",
    "\t\t\t   X:list # the texts to iterate over\n",
    "\t\t\t   ):\n",
    "\t\t\"\"\" Iterator to yield texts one by one. \"\"\"\n",
    "\t\tfor text in X:\n",
    "\t\t\tif text is None:\n",
    "\t\t\t\tyield ''\n",
    "\t\t\telse:\n",
    "\t\t\t\tyield str(text)\n",
    "\n",
    "\tdef _fit_textstats(self, \n",
    "\t\t\t\t\tdoc:spacy.tokens.doc.Doc, # the spaCy document to fit text statistics for \n",
    "\t\t\t\t\ttokens:list # the tokens to fit text statistics for\n",
    "\t\t\t\t\t) -> list: # the text statistics for the document\n",
    "\t\t\"\"\" Fit textstats for a document.\"\"\"\n",
    "\t\ttextstats = []\n",
    "\t\ttextstats.append(textstat.monosyllabcount(doc.text)) # monosyllable relfreq\n",
    "\t\ttextstats.append(textstat.polysyllabcount(doc.text)) # polysyllable relfreq\n",
    "\t\ttextstats.append(len(tokens)) # token count\n",
    "\t\ttextstats.append(len(list(doc.sents))) # sentence count\n",
    "\t\ttextstats.append(len(set([token.lower() for token in tokens]))) # unique_tokens_count\n",
    "\t\tif len(list(doc.sents)) > 0:\n",
    "\t\t\ttextstats.append(len(tokens)/len(list(doc.sents))) # average sentence length\n",
    "\t\telse:\n",
    "\t\t\ttextstats.append(0) # average sentence length is zero when no sentences\n",
    "\t\treturn textstats\n",
    "\n",
    "\tdef _spacy_tokenize(self, \n",
    "\t\t\t\t\t doc:spacy.tokens.doc.Doc # the spaCy document to tokenize\n",
    "\t\t\t\t\t ) -> tuple[list[str], list[str]]: # returns lists of tokens and part of speech tags\n",
    "\t\treturn [token.text for token in doc if not token.is_space], [token.pos_ for token in doc if not token.is_space]\n",
    "\t\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\t\"\"\" Fit is implemented, but does nothing. \"\"\"\n",
    "\t\treturn self\n",
    "\t\n",
    "\tdef transform(self, X):\n",
    "\t\t\"\"\" Preprocess the texts using spaCy and populate the feature store ready for use later in a pipeline. \"\"\"\n",
    "\n",
    "\t\ttokens = self.feature_store.get_tokens_from_texts(X)\n",
    "\t\tif any(x is None for x in tokens):\n",
    "\t\t\tfor doc in self.nlp.pipe(self._iterator(X), batch_size=self.batch_size, n_process=self.n_process):\n",
    "\t\t\t\ttokens, pos = self._spacy_tokenize(doc)\n",
    "\t\t\t\ttextstats = self._fit_textstats(doc, tokens)\n",
    "\t\t\t\tself.feature_store.update(doc.text, tokens, pos, textstats)\n",
    "\t\telse:\n",
    "\t\t\t# all the tokens are already in the feature store so no need to reprocess\n",
    "\t\t\tpass\n",
    "\t\treturn X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "feature_store = TextFeatureStore('../test-data/test_preprocess')\n",
    "spacy_preprocessor = SpacyPreprocessor(feature_store=feature_store)\n",
    "\n",
    "spacy_preprocessor.fit([''])\n",
    "spacy_preprocessor.transform([''])\n",
    "assert spacy_preprocessor.feature_store.get('')['tokens'] == []\n",
    "\n",
    "spacy_preprocessor.fit(['Hello, world!'])\n",
    "spacy_preprocessor.transform(['Hello, world!'])\n",
    "assert spacy_preprocessor.feature_store.get('Hello, world!')['tokens'] == ['Hello', ',', 'world', '!']\n",
    "assert spacy_preprocessor.feature_store.get('Hello, world!')['pos'] == ['INTJ', 'PUNCT', 'NOUN', 'PUNCT']\n",
    "spacy_preprocessor.feature_store.get('Hello, world!')['textstats']\n",
    "assert spacy_preprocessor.feature_store.get('Hello, world!')['textstats'] == [1, 0, 4, 1, 4, 4.0]\n",
    "# checking that the feature store has been populated ...\n",
    "assert spacy_preprocessor.feature_store.get('Hello, world!') == {'tokens': ['Hello', ',', 'world', '!'], 'pos': ['INTJ', 'PUNCT', 'NOUN', 'PUNCT'], 'textstats': [1, 0, 4, 1, 4, 4.0], 'embeddings': None, 'lexicons': None}\n",
    "del feature_store\n",
    "os.remove('../test-data/test_preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textplumber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
