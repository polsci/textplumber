{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textstats\n",
    "\n",
    "> Extract document-level statistics as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp textstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from textplumber.store import TextFeatureStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextstatsTransformer(BaseEstimator, TransformerMixin):\n",
    "\t\"\"\" Sci-kit Learn pipeline component to extract document-level text statistics based on the textstat library and pre-computed counts. \n",
    "\t\tThis component should be used after the SpacyPreprocessor component with the same feature store. \n",
    "\t\tThe statistics currently available are monosyllable count, polysyllable count, token count, sentence count, unique tokens count and average sentence length. \"\"\"\n",
    "\tdef __init__(self, \n",
    "\t\t\t  \tfeature_store: TextFeatureStore, # the feature store to use\n",
    "\t\t\t\tcolumns = ['monosyll_count', 'polysyll_count', 'token_count', 'sentence_count', 'unique_tokens_count', 'average_sentence_length']\n",
    "\t\t\t\t#scale: bool = True, # whether to scale the features - not implemented yet\n",
    "\t\t\t\t):\n",
    "\t\tself.feature_store = feature_store\n",
    "\t\t#self.scale = scale\n",
    "\t\t# check that passed columns matches these ...\n",
    "\t\tpossible_columns = ['monosyll_count', 'polysyll_count', 'token_count', 'sentence_count', 'unique_tokens_count', 'average_sentence_length']\n",
    "\t\tfor col in columns:\n",
    "\t\t\tif col not in possible_columns:\n",
    "\t\t\t\traise ValueError(f\"Invalid column name: {col}. Possible columns are: {possible_columns}\")\n",
    "\t\tself.columns = columns\n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\t\"\"\" Fit is implemented but does nothing. \"\"\"\n",
    "\t\t#if self.scale:        \n",
    "\t\t#\tself.scaler_ = StandardScaler()\n",
    "\t\t#\tself.scaler_.fit(self.feature_store.get_textstats_from_texts(X, self.columns))\n",
    "\t\treturn self\n",
    "\t\n",
    "\tdef transform(self, X):\n",
    "\t\t\"\"\" Transforms the texts to a matrix of text statistics. \"\"\"\n",
    "\t\ttextstats = self.feature_store.get_textstats_from_texts(X, self.columns)\n",
    "\t\t#if self.scale:\n",
    "\t\t#\ttextstats = self.scaler_.transform(textstats)\n",
    "\t\treturn textstats\n",
    "\t\n",
    "\tdef get_feature_names_out(self, input_features=None):\n",
    "\t\t\"\"\" Get the feature names out from the text statistics. \"\"\"\n",
    "\t\treturn self.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from textplumber.preprocess import SpacyPreprocessor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "\n",
    "feature_store = TextFeatureStore('../test-data/test_textstats')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('spacy_preprocessor', SpacyPreprocessor(feature_store=feature_store)),\n",
    "    ('textstats_transformer', TextstatsTransformer(feature_store=feature_store))\n",
    "])\n",
    "\n",
    "X = pipeline.fit_transform(['Hello, world!'])\n",
    "id = pipeline.named_steps['textstats_transformer'].get_feature_names_out().index('monosyll_count')\n",
    "assert X[0][id] == 1\n",
    "\n",
    "os.remove('../test-data/test_textstats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
