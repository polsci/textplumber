{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# report\n",
    "\n",
    "> Functions to aide reporting - need to document and show example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def preview_splits(X_train, y_train, X_test, y_test, label_names):\n",
    "    print(f\"Train: {len(X_train)} samples, {len(set(y_train))} classes\")\n",
    "    train_label_counts = pd.DataFrame(y_train).value_counts().to_frame()\n",
    "    train_label_counts.insert(0, 'label_name', label_names)\n",
    "    display(train_label_counts)\n",
    "    print(f\"Test: {len(X_test)} samples, {len(set(y_test))} classes\")\n",
    "    test_label_counts = pd.DataFrame(y_test).value_counts().to_frame()\n",
    "    test_label_counts.insert(0, 'label_name', label_names)\n",
    "    display(test_label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def preview_label_counts(df, label_column, label_names):\n",
    "    summary = pd.DataFrame(df.groupby([label_column])[label_column].count())\n",
    "    summary.columns = ['count']\n",
    "    summary.insert(0, 'label_name', label_names)\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def preview_text_field(text: str, # Text to preview\n",
    "\t\t\t\t\t   width: int = 80 # Width to wrap the text to\n",
    "\t\t\t\t\t   ):\n",
    "\t\"\"\" Preview a text field, wrapping the text to 80 characters \"\"\"\n",
    "\tfor line in text.split(\"\\r\\n\"):\n",
    "\t\tprint(textwrap.fill(line, width=width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def preview_row_text(df: pd.DataFrame, # DataFrame containing the data\n",
    "\t\t\t\t\t selected_index: int, # Index of the row to preview \n",
    "\t\t\t\t\t text_column: str = 'text', # column name for text field\n",
    "\t\t\t\t\t limit: int = -1 # Limit the length of the text field\n",
    "\t\t\t\t\t ):\n",
    "\t\"\"\" Preview the text fields of a row in the DataFrame \"\"\"\n",
    "\n",
    "\t# get row where index is selected_index as a frame - not iloc\n",
    "\tsummary = df.loc[selected_index].to_frame().drop(text_column)\n",
    "\tsummary.columns = ['Value']\n",
    "\tsummary.index.name = 'Attribute'\n",
    "\tdisplay(summary)\n",
    "\n",
    "\tprint(f\"{text_column}:\")\n",
    "\ttext = df[text_column].loc[selected_index]\n",
    "\tif limit > 1:\n",
    "\t\tif len(text) > limit:\n",
    "\t\t\ttext = text[:limit] + \"...\"\n",
    "\tpreview_text_field(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_confusion_matrix(y_test, y_predicted, target_classes, target_names):\n",
    "\t# Compute confusion matrix\n",
    "\tcm = confusion_matrix(y_test, y_predicted, labels=target_classes)\n",
    "\n",
    "\t# Compute row and column totals\n",
    "\trow_totals = cm.sum(axis=1)  # Row totals\n",
    "\tcol_totals = cm.sum(axis=0)  # Column totals\n",
    "\toverall_total = cm.sum()  # Overall total\n",
    "\n",
    "\t# Compute normalized proportions\n",
    "\tcm_normalized = cm / cm.sum(axis=1, keepdims=True)  # Normalize rows (proportions)\n",
    "\n",
    "\t# Combine counts and proportions into annotations\n",
    "\tannotations = np.empty_like(cm).astype(str)\n",
    "\tfor i in range(cm.shape[0]):\n",
    "\t\tfor j in range(cm.shape[1]):\n",
    "\t\t\tannotations[i, j] = f\"{cm[i, j]}\\n({cm_normalized[i, j]:.2f})\"\n",
    "\n",
    "\t# Create updated axis labels with totals\n",
    "\txticklabels_with_totals = [f\"{label}\\n(Total: {total})\" for label, total in zip(target_names, col_totals)]\n",
    "\tyticklabels_with_totals = [f\"{label} (Total: {total})\" for label, total in zip(target_names, row_totals)]\n",
    "\n",
    "\t# Create heatmap without totals in the matrix\n",
    "\tfig, ax = plt.subplots(figsize=(10, 8))\n",
    "\tsns.heatmap(cm,\n",
    "\t\t\t\tannot=annotations,\n",
    "\t\t\t\tfmt='',\n",
    "\t\t\t\tcmap='Blues',\n",
    "\t\t\t\txticklabels=xticklabels_with_totals,\n",
    "\t\t\t\tyticklabels=yticklabels_with_totals,\n",
    "\t\t\t\tcbar=True)\n",
    "\n",
    "\tplt.xlabel('Predicted Labels')\n",
    "\tplt.ylabel('Actual Labels')\n",
    "\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
