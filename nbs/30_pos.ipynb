{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pos\n",
    "\n",
    "> Extract parts of speech features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from textplumber.store import TextFeatureStore\n",
    "from textplumber.core import pass_tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class POSVectorizer(BaseEstimator, TransformerMixin):\n",
    "\t\"\"\" Sci-kit Learn pipeline component to extract parts of speech tag features. This component should be used after the SpacyPreprocessor component with the same feature store.\n",
    "\t\tThe component gets the tokens from the feature store and returns a matrix of counts (via CountVectorizer). \"\"\"\n",
    "\t\n",
    "\tdef __init__(self, \n",
    "\t\t\t\tfeature_store: TextFeatureStore, # the feature store to use - this should be the same feature store used in the SpacyPreprocessor component\n",
    "\t\t\t\tngram_range:tuple = (1, 1), # the ngram range to use (min_n, max_n) - passed to CountVectorizer\n",
    "\t\t\t\tvocabulary:list|None = None, # list of tokens to use - passed to CountVectorizer\n",
    "\t\t\t\t# scale: bool = False, # whether to normalize the counts - not implemented yet\n",
    "\t\t\t\t):\n",
    "\t\tself.feature_store = feature_store\n",
    "\t\tself.ngram_range = ngram_range\n",
    "\t\tself.vocabulary = vocabulary\n",
    "\t\t# self.scale = scale\n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\tself.vectorizer_ = CountVectorizer(tokenizer=pass_tokens,\n",
    "\t\t\t\t\t\t\t\t\tlowercase=False, \n",
    "\t\t\t\t\t\t\t\t\tstop_words=None, \n",
    "\t\t\t\t\t\t\t\t\ttoken_pattern=None, \n",
    "\t\t\t\t\t\t\t\t\tmin_df=1,\n",
    "\t\t\t\t\t\t\t\t\tmax_df=1.0,\n",
    "\t\t\t\t\t\t\t\t\tmax_features=None,\n",
    "\t\t\t\t\t\t\t\t\tngram_range=self.ngram_range,\n",
    "\t\t\t\t\t\t\t\t\tvocabulary= self.vocabulary)\n",
    "\t\tself.vectorizer_.fit(self.feature_store.get_pos_from_texts(X), y)\n",
    "\t\t# self.scaler_ = Normalizer(norm='l1')\n",
    "\t\treturn self\n",
    "\t\n",
    "\tdef transform(self, X):\n",
    "\t\tdocs_pos = self.feature_store.get_pos_from_texts(X)\n",
    "\t\tdocs_pos = self.vectorizer_.transform(docs_pos)\n",
    "\t\t#if self.scale:\n",
    "\t\t#\tdocs_pos = self.scaler_.transform(docs_pos)\n",
    "\t\treturn docs_pos\n",
    "\t\n",
    "\tdef get_feature_names_out(self, input_features=None):\n",
    "\t\treturn self.vectorizer_.get_feature_names_out(input_features)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from textplumber.preprocess import SpacyPreprocessor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "\n",
    "feature_store = TextFeatureStore('test_pos.sqlite')\n",
    "spacy_preprocessor = SpacyPreprocessor(feature_store=feature_store)\n",
    "spacy_pos_vectorizer = POSVectorizer(feature_store=feature_store)\n",
    "pipeline = Pipeline([\n",
    "    ('spacy_preprocessor', spacy_preprocessor),\n",
    "    ('spacy_pos_vectorizer', spacy_pos_vectorizer)\n",
    "])\n",
    "pipeline.fit(['Hello, world!'])\n",
    "X = pipeline.transform(['Hello, world!'])\n",
    "id = spacy_pos_vectorizer.get_feature_names_out().tolist().index('NOUN')\n",
    "assert X.todense()[0, id] == 1\n",
    "id = spacy_pos_vectorizer.get_feature_names_out().tolist().index('PUNCT')\n",
    "assert X.todense()[0, id] == 2\n",
    "id = spacy_pos_vectorizer.get_feature_names_out().tolist().index('INTJ')\n",
    "assert X.todense()[0, id] == 1\n",
    "del feature_store\n",
    "del pipeline\n",
    "\n",
    "os.remove('test_pos.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textplumber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
