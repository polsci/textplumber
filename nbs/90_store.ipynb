{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store\n",
    "\n",
    "> Store text features to avoid recomputing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pickle\n",
    "import os\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFeatureStore:\n",
    "\t\"\"\" A class to store features extracted for a text classification pipeline and cache them to disk to avoid recomputing them. \"\"\"\n",
    "\tdef __init__(self, \n",
    "\t\t\t  \t path:str = None # where to save a pickle file to persist the feature store between runs\n",
    "\t\t\t\t ):\n",
    "\t\tself.texts = {}\n",
    "\t\tself.path = path\n",
    "\n",
    "\t\tif self.path is not None:\n",
    "\t\t\t# self.path shouuld be a file - make dirs if it doesn't exist\n",
    "\t\t\tif not os.path.exists(self.path):\n",
    "\t\t\t\tos.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "\n",
    "\tdef empty(self):\n",
    "\t\t\"\"\"Initialize the feature store.\"\"\"\n",
    "\t\tself.texts = {}\n",
    "\n",
    "\tdef save(self):\n",
    "\t\t\"\"\"Save the feature store to disk.\"\"\"\n",
    "\t\tif self.path is not None:\n",
    "\t\t\twith open(self.path, 'wb') as f:\n",
    "\t\t\t\tpickle.dump(self.texts, f)\n",
    "\n",
    "\tdef load(self):\n",
    "\t\t\"\"\"Load the feature store to disk.\"\"\"\n",
    "\t\tif self.path is not None and os.path.exists(self.path):\n",
    "\t\t\twith open(self.path, 'rb') as f:\n",
    "\t\t\t\tself.texts = pickle.load(f)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef update(self, \n",
    "\t\t\t   text:str, # the text to update\n",
    "\t\t\t   tokens:list, # the tokens to update\n",
    "\t\t\t   pos:list, # the part of speech tags to update\n",
    "\t\t\t   textstats:list, # the text statistics to update\n",
    "\t\t\t   ):\n",
    "\t\t\"\"\"Update the feature store tokens, parts of speech tags and text statistics for a text.\"\"\"\n",
    "\t\thash = hashlib.md5(text.encode()).hexdigest()\n",
    "\t\tif hash not in self.texts:\n",
    "\t\t\tself.texts[hash] = {}\n",
    "\t\tself.texts[hash]['tokens'] = tokens\n",
    "\t\tself.texts[hash]['pos'] = pos\n",
    "\t\tself.texts[hash]['textstats'] = textstats\n",
    "\n",
    "\tdef update_embeddings(self, \n",
    "\t\t\t\t\t   \t  texts:str, # the texts to update \n",
    "\t\t\t\t\t\t  embeddings:list # the embeddings to update\n",
    "\t\t\t\t\t\t  ):\n",
    "\t\t\"\"\"Update the feature store with embeddings for a list of texts.\"\"\"\n",
    "\n",
    "\t\tfor i, text in enumerate(texts):\n",
    "\t\t\thash = hashlib.md5(text.encode()).hexdigest()\n",
    "\t\t\tif hash not in self.texts:\n",
    "\t\t\t\tself.texts[hash] = {}\n",
    "\t\t\tself.texts[hash]['embeddings'] = embeddings[i]\n",
    "\t\t\n",
    "\tdef get(self, \n",
    "\t\t \ttext:str, # the text to get features for\n",
    "\t\t\ttype:str = None # the type of features to get - 'tokens', 'pos', 'textstats', 'embeddings'\n",
    "\t\t\t) -> dict|list: # the features for the text\n",
    "\t\t\"\"\" Get features for a text.\"\"\"\n",
    "\n",
    "\t\thash = hashlib.md5(text.encode()).hexdigest()\n",
    "\t\tif type is None:\n",
    "\t\t\treturn self.texts.get(hash, None)\n",
    "\t\telif type in ['tokens', 'pos', 'textstats', 'embeddings']:\n",
    "\t\t\treturn self.texts.get(hash, {}).get(type, None)\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"Type {type} not in ['tokens', 'pos', 'textstats', 'embeddings']\")\n",
    "\t\t\n",
    "\tdef get_tokens_from_texts(self, \n",
    "\t\t\t\t\t\t      texts:list, # the texts to get tokens for\n",
    "\t\t\t\t\t\t\t  lowercase:bool = False # whether to lowercase the tokens\n",
    "\t\t\t\t\t\t\t  ) -> list: # the tokens for the texts\n",
    "\t\t\"\"\" Get tokens for a list of texts. \"\"\"\n",
    "\t\ttokens = []\n",
    "\t\tfor text in texts:\n",
    "\t\t\ttokens.append(self.get(text, 'tokens'))\n",
    "\t\tif lowercase == True:\n",
    "\t\t\ttokens = [[token.lower() for token in text] for text in tokens]\n",
    "\t\treturn tokens\n",
    "\t\n",
    "\tdef get_textstats_from_texts(self, \n",
    "\t\t\t\t\t\t\t  \t texts:list, # the texts to get text statistics for\n",
    "\t\t\t\t\t\t\t\t columns_out = ['monosyll_count', 'polysyll_count', 'token_count', 'sentence_count', 'unique_tokens_count', 'average_sentence_length'], # the text statistics to get\n",
    "\t\t\t\t\t\t\t\t columns_in = ['monosyll_count', 'polysyll_count', 'token_count', 'sentence_count', 'unique_tokens_count', 'average_sentence_length'] # the column name order - leaving for future use, but probably not needed\n",
    "\t\t\t\t\t\t\t\t ) -> list: # the text statistics for the texts\n",
    "\t\t\"\"\" Get text statistics for a list of texts. \"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tcolumns = [columns_in.index(col) for col in columns_out]\n",
    "\t\texcept ValueError:\n",
    "\t\t\traise ValueError(f\"Columns {columns_out} not in {columns_in}\")\n",
    "\t\ttextstats = []\n",
    "\t\tfor text in texts:\n",
    "\t\t\ttextstat = self.get(text, 'textstats')\n",
    "\t\t\ttextstats.append([textstat[i] for i in columns])\n",
    "\t\treturn textstats\n",
    "\t\n",
    "\tdef get_pos_from_texts(self, \n",
    "\t\t\t\t\t\t   texts:list, # the texts to get part of speech tags for\n",
    "\t\t\t\t\t\t   ) -> list: # the part of speech tags for the texts\n",
    "\t\t\"\"\" Get parts of speech for a list of texts. \"\"\"\n",
    "\t\tpos = []\n",
    "\t\tfor text in texts:\n",
    "\t\t\tpos.append(self.get(text, 'pos'))\n",
    "\t\treturn pos\n",
    "\n",
    "\tdef get_embeddings_from_texts(self, \n",
    "\t\t\t\t\t\t\t      texts:str # the texts to get embeddings for\n",
    "\t\t\t\t\t\t\t\t  ) -> list: # the embeddings for the texts\n",
    "\t\t\"\"\" Get embeddings for multiple texts. \"\"\"\n",
    "\t\tembeddings = []\n",
    "\t\tfor text in texts:\n",
    "\t\t\tembeddings.append(self.get(text, 'embeddings'))\n",
    "\t\treturn embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "text_feature_store = TextFeatureStore(path='pipeline/test_feature_store')\n",
    "text_feature_store.empty()\n",
    "text_feature_store.update('Hello, world!', ['Hello', 'world'], ['NNP', 'NN'], [1, 1])\n",
    "\n",
    "assert text_feature_store.get('Hello, world!') == {'tokens': ['Hello', 'world'], 'pos': ['NNP', 'NN'], 'textstats': [1, 1]}\n",
    "assert text_feature_store.get('Hello, world!', 'tokens') == ['Hello', 'world']\n",
    "assert text_feature_store.get('Hello, world!', 'pos') == ['NNP', 'NN']\n",
    "assert text_feature_store.get('Hello, world!', 'textstats') == [1, 1]\n",
    "\n",
    "text_feature_store.update_embeddings(['Hello, world!'], [[1, 2, 3]])\n",
    "assert text_feature_store.get('Hello, world!', 'embeddings') == [1, 2, 3]\n",
    "\n",
    "text_feature_store.empty()\n",
    "assert text_feature_store.get('Hello, world!', 'tokens') is None\n",
    "assert text_feature_store.get('Hello, world!', 'pos') is None\n",
    "assert text_feature_store.get('Hello, world!', 'textstats') is None\n",
    "assert text_feature_store.get_embeddings_from_texts(['Hello, world!']) == [None]\n",
    "\n",
    "text_feature_store.update('Hello, world!', ['Hello', 'world'], ['NNP', 'NN'], [1, 1])\n",
    "text_feature_store.update_embeddings(['Hello, world!'], [[1, 2, 3]])\n",
    "assert text_feature_store.get_embeddings_from_texts(['Hello, world!']) == [[1, 2, 3]]\n",
    "assert text_feature_store.get_embeddings_from_texts(['Hello, world!', 'Hi, world!']) == [[1, 2, 3], None]\n",
    "\n",
    "text_feature_store.save()\n",
    "assert os.path.exists('pipeline/test_feature_store')\n",
    "\n",
    "text_feature_store.empty()\n",
    "assert text_feature_store.get('Hello, world!', 'tokens') is None\n",
    "\n",
    "text_feature_store.load()\n",
    "assert text_feature_store.get('Hello, world!') == {'tokens': ['Hello', 'world'], 'pos': ['NNP', 'NN'], 'textstats': [1, 1], 'embeddings': [1, 2, 3]}\n",
    "text_feature_store.empty()\n",
    "os.remove('pipeline/test_feature_store')\n",
    "os.rmdir('pipeline')\n",
    "\n",
    "text_feature_store = TextFeatureStore()\n",
    "text_feature_store.update('Hello, world!', ['Hello', 'world'], ['NNP', 'NN'], [1, 1])\n",
    "assert text_feature_store.get('Hello, world!') == {'tokens': ['Hello', 'world'], 'pos': ['NNP', 'NN'], 'textstats': [1, 1]}\n",
    "text_feature_store.save()\n",
    "text_feature_store = text_feature_store.load()\n",
    "assert text_feature_store.get('Hello, world!') == {'tokens': ['Hello', 'world'], 'pos': ['NNP', 'NN'], 'textstats': [1, 1]}\n",
    "\n",
    "text_feature_store.update('Hello, world!', ['Hello', 'world'], ['NNP', 'NN'], [1, 1, 2, 1, 2, 1])\n",
    "assert text_feature_store.get_textstats_from_texts(['Hello, world!']) == [[1, 1, 2, 1, 2, 1]]\n",
    "assert text_feature_store.get_textstats_from_texts(['Hello, world!'], columns_out=['monosyll_count', 'polysyll_count']) == [[1, 1]]\n",
    "\n",
    "# this should raise a value error\n",
    "try:\n",
    "    textstats = text_feature_store.get_textstats_from_texts(['Hello, world!'], columns_out=['dr_dre'])\n",
    "except ValueError as e:\n",
    "    assert True\n",
    "else:\n",
    "    assert False, \"Expected ValueError not raised\"\n",
    "\n",
    "del text_feature_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
