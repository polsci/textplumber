{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store\n",
    "\n",
    "> Store text features to avoid recomputing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pickle\n",
    "import os\n",
    "import hashlib\n",
    "import sqlite3\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFeatureStore:\n",
    "\t\"\"\" A class to store features extracted for a text classification pipeline and cache them to disk to avoid recomputing them. \"\"\"\n",
    "\tdef __init__(self, \n",
    "\t\t\t  \t path:str  # where to save sqllite db to persist the feature store between runs\n",
    "\t\t\t\t ):\n",
    "\t\tself.texts = {}\n",
    "\t\tself.path = path\n",
    "\n",
    "\t\tif self.path is not None:\n",
    "\t\t\tif os.path.dirname(self.path) != '':\n",
    "\t\t\t\t# self.path shouuld be a file - make dirs if it doesn't exist\n",
    "\t\t\t\tif not os.path.exists(os.path.dirname(self.path)):\n",
    "\t\t\t\t\tos.makedirs(os.path.dirname(self.path))\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"The feature store requires a path\")\n",
    "\n",
    "\t\twith sqlite3.connect(self.path) as conn:\n",
    "\t\t\t#conn.execute(\"PRAGMA journal_mode=WAL\")\n",
    "\t\t\tconn.execute('''CREATE TABLE IF NOT EXISTS texts\n",
    "\t\t\t\t(hash TEXT PRIMARY KEY, tokens BLOB NOT NULL, pos BLOB NOT NULL, textstats BLOB NOT NULL) WITHOUT ROWID;''')\n",
    "\t\t\tconn.execute('''CREATE TABLE IF NOT EXISTS embeddings\n",
    "\t\t\t\t(hash TEXT PRIMARY KEY, embeddings BLOB NOT NULL) WITHOUT ROWID;''')\t\t\n",
    "\t\t\tconn.execute('''CREATE TABLE IF NOT EXISTS lexicons\n",
    "\t\t\t\t(hash TEXT PRIMARY KEY, lexicons BLOB NOT NULL) WITHOUT ROWID;''')\t\n",
    "\t\t\tconn.commit()\n",
    "\n",
    "\tdef empty(self):\n",
    "\t\t\"\"\"Clear the contents of the feature store.\"\"\"\n",
    "\t\twith sqlite3.connect(self.path) as conn:\n",
    "\t\t\tconn.execute('''DELETE FROM texts;''')\n",
    "\t\t\tconn.execute('''DELETE FROM embeddings;''')\n",
    "\t\t\tconn.execute('''DELETE FROM lexicons;''')\n",
    "\t\t\tconn.commit()\n",
    "\n",
    "\t# def save(self):\n",
    "\t# \t\"\"\"Save the feature store to disk.\"\"\"\n",
    "\t# \tif self.path is not None:\n",
    "\t# \t\twith open(self.path, 'wb') as f:\n",
    "\t# \t\t\tpickle.dump(self.texts, f)\n",
    "\n",
    "\tdef update(self, \n",
    "\t\t\t   text:str, # the text to update\n",
    "\t\t\t   tokens:list, # the tokens to update\n",
    "\t\t\t   pos:list, # the part of speech tags to update\n",
    "\t\t\t   textstats:list, # the text statistics to update\n",
    "\t\t\t   ):\n",
    "\t\t\"\"\"Update the feature store tokens, parts of speech tags and text statistics for a text.\"\"\"\n",
    "\t\thash = hashlib.md5(text.encode()).hexdigest()\n",
    "\t\twith sqlite3.connect(self.path) as conn:\n",
    "\t\t\tconn.execute('''\n",
    "\t\t\t\tINSERT OR REPLACE INTO texts (hash, tokens, pos, textstats)\n",
    "\t\t\t\tVALUES (?, ?, ?, ?)\n",
    "\t\t\t''', (hash, pickle.dumps(tokens), pickle.dumps(pos), pickle.dumps(textstats)))\n",
    "\t\t\tconn.commit()\n",
    "\n",
    "\tdef update_embeddings(self, \n",
    "\t\t\t\t\t   \t  texts:str, # the texts to update \n",
    "\t\t\t\t\t\t  embeddings:list # the embeddings to update\n",
    "\t\t\t\t\t\t  ):\n",
    "\t\t\"\"\"Update the feature store with embeddings for a list of texts.\"\"\"\n",
    "\n",
    "\t\twith sqlite3.connect(self.path) as conn:\n",
    "\t\t\tfor i, text in enumerate(texts):\n",
    "\t\t\t\thash = hashlib.md5(text.encode()).hexdigest()\n",
    "\t\t\t\tconn.execute('''\n",
    "\t\t\t\t\tINSERT OR REPLACE INTO embeddings (hash, embeddings) \n",
    "\t\t\t\t\tVALUES (?, ?)\n",
    "\t\t\t\t''', (hash, pickle.dumps(embeddings[i])))\n",
    "\t\t\t\tconn.commit()\n",
    "\n",
    "\tdef update_lexicons(self, \n",
    "\t\t\t\t\t   \t  texts:str, # the texts to update \n",
    "\t\t\t\t\t\t  lexicons:list # the lexicon scores to update\n",
    "\t\t\t\t\t\t  ):\n",
    "\t\t\"\"\"Update the feature store with lexicon features for a list of texts.\"\"\"\n",
    "\n",
    "\t\twith sqlite3.connect(self.path) as conn:\n",
    "\t\t\tfor i, text in enumerate(texts):\n",
    "\t\t\t\thash = hashlib.md5(text.encode()).hexdigest()\n",
    "\t\t\t\tconn.execute('''\n",
    "\t\t\t\t\tINSERT OR REPLACE INTO lexicons (hash, lexicons) \n",
    "\t\t\t\t\tVALUES (?, ?)\n",
    "\t\t\t\t''', (hash, pickle.dumps(lexicons[i])))\n",
    "\t\t\t\tconn.commit()\n",
    "\n",
    "\tdef get(self, \n",
    "\t\t \ttext:str, # the text to get features for\n",
    "\t\t\ttype:str = None # the type of features to get - 'tokens', 'pos', 'textstats', 'embeddings', 'lexicons'\n",
    "\t\t\t) -> dict|list: # the features for the text\n",
    "\t\t\"\"\" Get features for a text.\"\"\"\n",
    "\n",
    "\t\tif type is not None and type not in ['tokens', 'pos', 'textstats', 'embeddings', 'lexicons']:\n",
    "\t\t\traise ValueError(f\"Type {type} not in ['tokens', 'pos', 'textstats', 'embeddings', 'lexicons']\")\n",
    "\n",
    "\t\twith sqlite3.connect(self.path) as conn:\n",
    "\t\t\thash = hashlib.md5(text.encode()).hexdigest()\n",
    "\t\t\tif type is None:\n",
    "\t\t\t\tcursor = conn.execute('SELECT * FROM texts WHERE hash=?', (hash,))\n",
    "\t\t\t\trow = cursor.fetchone()\n",
    "\t\t\t\tresult = {'tokens': pickle.loads(row[1]), 'pos': pickle.loads(row[2]), 'textstats': pickle.loads(row[3])}\n",
    "\t\t\t\tif row is None:\n",
    "\t\t\t\t\treturn None\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcursor = conn.execute('SELECT * FROM embeddings WHERE hash=?', (hash,))\n",
    "\t\t\t\t\trow = cursor.fetchone()\n",
    "\t\t\t\t\tif row is None:\n",
    "\t\t\t\t\t\tresult['embeddings'] = None\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tresult['embeddings'] = pickle.loads(row[1])\n",
    "\n",
    "\t\t\t\t\tcursor = conn.execute('SELECT * FROM lexicons WHERE hash=?', (hash,))\n",
    "\t\t\t\t\trow = cursor.fetchone()\n",
    "\t\t\t\t\tif row is None:\n",
    "\t\t\t\t\t\tresult['lexicons'] = None\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tresult['lexicons'] = pickle.loads(row[1])\n",
    "\t\t\t\t\treturn result\n",
    "\n",
    "\t\t\telif type in ['tokens', 'pos', 'textstats']:\n",
    "\t\t\t\tcursor = conn.execute(f'SELECT {type} FROM texts WHERE hash=?', (hash,))\n",
    "\t\t\t\trow = cursor.fetchone()\n",
    "\t\t\t\tif row is None:\n",
    "\t\t\t\t\treturn None\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treturn pickle.loads(row[0])\n",
    "\t\t\telif type == 'embeddings':\n",
    "\t\t\t\tcursor = conn.execute('SELECT embeddings FROM embeddings WHERE hash=?', (hash,))\n",
    "\t\t\t\trow = cursor.fetchone()\n",
    "\t\t\t\tif row is None:\n",
    "\t\t\t\t\treturn None\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treturn pickle.loads(row[0])\n",
    "\t\t\telif type == 'lexicons':\n",
    "\t\t\t\tcursor = conn.execute('SELECT lexicons FROM lexicons WHERE hash=?', (hash,))\n",
    "\t\t\t\trow = cursor.fetchone()\n",
    "\t\t\t\tif row is None:\n",
    "\t\t\t\t\treturn None\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treturn pickle.loads(row[0])\n",
    "\n",
    "\tdef get_features_from_texts_by_type(self,\n",
    "\t\t\t\t\t\t\t texts:list, # the texts to get features for\n",
    "\t\t\t\t\t\t\t type:str # the type of features to get - 'tokens', 'pos', 'textstats', 'embeddings', 'lexicons'\n",
    "\t\t\t\t\t\t\t ) -> list: # the features for the texts\n",
    "\t\t\"\"\" Get features for a list of texts.\"\"\"\n",
    "\t\thashes = [hashlib.md5(text.encode()).hexdigest() for text in texts]\n",
    "\n",
    "\t\tif type not in ['tokens', 'pos', 'textstats', 'embeddings', 'lexicons']:\n",
    "\t\t\traise ValueError(f\"Type {type} not in ['tokens', 'pos', 'textstats', 'embeddings', 'lexicons']\")\n",
    "\n",
    "\t\twith sqlite3.connect(self.path) as conn:\n",
    "\t\t\tif type == 'embeddings':\n",
    "\t\t\t\tcursor = conn.execute(f'SELECT hash, embeddings FROM embeddings WHERE hash IN ({\", \".join([\"?\"] * len(hashes))})', hashes)\n",
    "\t\t\telif type == 'lexicons':\n",
    "\t\t\t\tcursor = conn.execute(f'SELECT hash, lexicons FROM lexicons WHERE hash IN ({\", \".join([\"?\"] * len(hashes))})', hashes)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcursor = conn.execute(f'SELECT hash, {type} FROM texts WHERE hash IN ({\", \".join([\"?\"] * len(hashes))})', hashes)\n",
    "\t\t\tfeatures = [None] * len(hashes)\n",
    "\n",
    "\t\t\tpositions = {}\n",
    "\t\t\tfor i, hash in enumerate(hashes):\n",
    "\t\t\t\tif hash not in positions:\n",
    "\t\t\t\t\tpositions[hash] = []\n",
    "\t\t\t\tpositions[hash].append(i)\n",
    "\n",
    "\t\t\tfor row in cursor:\n",
    "\t\t\t\tif row[0] in hashes:\n",
    "\t\t\t\t\tfor position in positions[row[0]]:\n",
    "\t\t\t\t\t\tfeatures[position] = pickle.loads(row[1])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise ValueError(f\"Hash {row[0]} not in {hashes}\")\n",
    "\t\t\t\n",
    "\t\treturn features\n",
    "\n",
    "\tdef get_tokens_from_texts(self, \n",
    "\t\t\t\t\t\t      texts:list, # the texts to get tokens for\n",
    "\t\t\t\t\t\t\t  lowercase:bool = False, # whether to lowercase the tokens\n",
    "\t\t\t\t\t\t\t  min_token_length:int = 0, # the minimum token length to include\n",
    "\t\t\t\t\t\t\t  remove_punctuation:bool = False, # whether to remove punctuation\n",
    "\t\t\t\t\t\t\t  remove_numbers:bool = False, # whether to remove numbers\n",
    "\t\t\t\t\t\t\t  ) -> list: # the tokens for the texts\n",
    "\t\t\"\"\" Get tokens for a list of texts. \"\"\"\n",
    "\t\ttokens = self.get_features_from_texts_by_type(texts, 'tokens')\n",
    "\t\tif lowercase == True:\n",
    "\t\t\ttokens = [[token.lower() for token in text] for text in tokens]\n",
    "\t\tif min_token_length > 0:\n",
    "\t\t\ttokens = [[token for token in text if len(token) >= min_token_length] for text in tokens]\n",
    "\t\tif remove_punctuation == True:\n",
    "\t\t\ttokens = [[token for token in text if token.strip(string.punctuation)] for text in tokens]\n",
    "\t\tif remove_numbers == True:\n",
    "\t\t\ttokens = [[token for token in text if not token.isdigit()] for text in tokens]\n",
    "\t\treturn tokens\n",
    "\t\n",
    "\tdef get_textstats_from_texts(self, \n",
    "\t\t\t\t\t\t\t  \t texts:list, # the texts to get text statistics for\n",
    "\t\t\t\t\t\t\t\t columns_out = ['monosyll_count', 'polysyll_count', 'token_count', 'sentence_count', 'unique_tokens_count', 'average_sentence_length'], # the text statistics to get\n",
    "\t\t\t\t\t\t\t\t columns_in = ['monosyll_count', 'polysyll_count', 'token_count', 'sentence_count', 'unique_tokens_count', 'average_sentence_length'] # the column name order - leaving for future use, but probably not needed\n",
    "\t\t\t\t\t\t\t\t ) -> list: # the text statistics for the texts\n",
    "\t\t\"\"\" Get text statistics for a list of texts. \"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tcolumns = [columns_in.index(col) for col in columns_out]\n",
    "\t\texcept ValueError:\n",
    "\t\t\traise ValueError(f\"Columns {columns_out} not in {columns_in}\")\n",
    "\n",
    "\t\ttextstats = self.get_features_from_texts_by_type(texts, 'textstats')\n",
    "\t\ttextstats = [[textstat[i] for i in columns] for textstat in textstats]\n",
    "\t\treturn textstats\n",
    "\t\n",
    "\tdef get_pos_from_texts(self, \n",
    "\t\t\t\t\t\t   texts:list, # the texts to get part of speech tags for\n",
    "\t\t\t\t\t\t   ) -> list: # the part of speech tags for the texts\n",
    "\t\t\"\"\" Get parts of speech for a list of texts. \"\"\"\n",
    "\t\tpos = self.get_features_from_texts_by_type(texts, 'pos')\n",
    "\t\treturn pos\n",
    "\n",
    "\tdef get_embeddings_from_texts(self, \n",
    "\t\t\t\t\t\t\t      texts:str # the texts to get embeddings for\n",
    "\t\t\t\t\t\t\t\t  ) -> list: # the embeddings for the texts\n",
    "\t\t\"\"\" Get embeddings for multiple texts. \"\"\"\n",
    "\n",
    "\t\tembeddings = self.get_features_from_texts_by_type(texts, 'embeddings')\n",
    "\t\treturn embeddings\n",
    "\t\n",
    "\tdef get_lexicons_from_texts(self,\n",
    "\t\t\t\t\t\t\t texts:str # the texts to get lexicons for\n",
    "\t\t\t\t\t\t\t ) -> list:\n",
    "\t\t\"\"\" Get lexicons for multiple texts. \"\"\"\n",
    "\t\tlexicons = self.get_features_from_texts_by_type(texts, 'lexicons')\n",
    "\t\treturn lexicons\n",
    "\t\n",
    "\tdef dump(self):\n",
    "\t\t\"\"\" show the contents of the feature store (intended for debugging/development) \"\"\"\n",
    "\t\twith sqlite3.connect(self.path) as conn:\n",
    "\t\t\tcursor = conn.execute('SELECT * FROM texts')\n",
    "\t\t\tfor row in cursor:\n",
    "\t\t\t\tprint(row[0], pickle.loads(row[1]), pickle.loads(row[2]), pickle.loads(row[3]))\n",
    "\t\t\tcursor = conn.execute('SELECT * FROM embeddings')\n",
    "\t\t\tfor row in cursor:\n",
    "\t\t\t\tprint(row[0], pickle.loads(row[1]))\n",
    "\t\t\tcursor = conn.execute('SELECT * FROM lexicons')\n",
    "\t\t\tfor row in cursor:\n",
    "\t\t\t\tprint(row[0], pickle.loads(row[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "text_feature_store = TextFeatureStore('test_store.sqlite')\n",
    "\n",
    "assert os.path.exists('test_store.sqlite')\n",
    "\n",
    "text_feature_store.empty()\n",
    "text_feature_store.update('Hello, world!', ['Hello', 'world'], ['INTJ', 'NOUN'], [1, 1])\n",
    "\n",
    "assert text_feature_store.get('Hello, world!') == {'tokens': ['Hello', 'world'], 'pos': ['INTJ', 'NOUN'], 'textstats': [1, 1], 'embeddings': None, 'lexicons': None}\n",
    "assert text_feature_store.get('Hello, world!', 'tokens') == ['Hello', 'world']\n",
    "assert text_feature_store.get('Hello, world!', 'pos') == ['INTJ', 'NOUN']\n",
    "assert text_feature_store.get('Hello, world!', 'textstats') == [1, 1]\n",
    "\n",
    "text_feature_store.update_embeddings(['Hello, world!'], [[1, 2, 3]])\n",
    "assert text_feature_store.get('Hello, world!', 'embeddings') == [1, 2, 3]\n",
    "\n",
    "text_feature_store.update_lexicons(['Hello, world!'], [[9, 11, 13]])\n",
    "assert text_feature_store.get('Hello, world!', 'lexicons') == [9, 11, 13]\n",
    "\n",
    "text_feature_store.update('Hello, world 123!', ['Hello', 'world', '123'], ['INTJ', 'NOUN'], [1, 1])\n",
    "text_feature_store.update('Hello, world 456!', ['Hello', 'world', '456'], ['INTJ', 'NOUN'], [1, 1])\n",
    "assert text_feature_store.get_tokens_from_texts(['Hello, world!', 'Hello, world 123!', 'Hello, world 456!']) == [['Hello', 'world'], ['Hello', 'world', '123'], ['Hello', 'world', '456']]\n",
    "\n",
    "text_feature_store.empty()\n",
    "assert text_feature_store.get('Hello, world!', 'tokens') is None\n",
    "assert text_feature_store.get('Hello, world!', 'pos') is None\n",
    "assert text_feature_store.get('Hello, world!', 'textstats') is None\n",
    "assert text_feature_store.get_embeddings_from_texts(['Hello, world!']) == [None]\n",
    "assert text_feature_store.get_lexicons_from_texts(['Hello, world!']) == [None]\n",
    "\n",
    "text_feature_store.update('Hello, world!', ['Hello', 'world'], ['INTJ', 'NOUN'], [1, 1])\n",
    "text_feature_store.update_embeddings(['Hello, world!'], [[1, 2, 3]])\n",
    "assert text_feature_store.get_embeddings_from_texts(['Hello, world!']) == [[1, 2, 3]]\n",
    "assert text_feature_store.get_embeddings_from_texts(['Hello, world!', 'Hi, world!']) == [[1, 2, 3], None]\n",
    "\n",
    "text_feature_store.update_lexicons(['Hello, world!'], [[9, 11, 13]])\n",
    "assert text_feature_store.get_lexicons_from_texts(['Hello, world!']) == [[9, 11, 13]]\n",
    "assert text_feature_store.get_lexicons_from_texts(['Hello, world!', 'Hi, world!']) == [[9, 11, 13], None]\n",
    "\n",
    "text_feature_store.empty()\n",
    "assert text_feature_store.get('Hello, world!', 'tokens') is None\n",
    "\n",
    "text_feature_store.update('A Hello, world!', ['A', 'Hello', ',', 'world', '!'], ['INTJ', 'PUNCT', 'NOUN', 'PUNCT'], [1, 1, 2, 1, 2, 1])\n",
    "assert text_feature_store.get_tokens_from_texts(['A Hello, world!']) == [['A', 'Hello', ',', 'world', '!']]\n",
    "assert text_feature_store.get_tokens_from_texts(['A Hello, world!'], lowercase=True) == [['a', 'hello', ',', 'world', '!']]\n",
    "assert text_feature_store.get_tokens_from_texts(['A Hello, world!'], min_token_length=2) == [['Hello', 'world']]\n",
    "assert text_feature_store.get_tokens_from_texts(['A Hello, world!'], remove_punctuation=True) == [['A', 'Hello', 'world']]\n",
    "\n",
    "text_feature_store.empty()\n",
    "text_feature_store.update('', [], [], [0, 0, 0, 0, 0, 0])\n",
    "assert text_feature_store.get('') == {'tokens': [], 'pos': [], 'textstats': [0, 0, 0, 0, 0, 0], 'embeddings': None, 'lexicons': None}\n",
    "\n",
    "# text_feature_store = TextFeatureStore()\n",
    "# text_feature_store.update('Hello, world!', ['Hello', 'world'], ['NNP', 'NN'], [1, 1])\n",
    "# assert text_feature_store.get('Hello, world!') == {'tokens': ['Hello', 'world'], 'pos': ['NNP', 'NN'], 'textstats': [1, 1]}\n",
    "# text_feature_store.save()\n",
    "# text_feature_store = text_feature_store.load()\n",
    "# assert text_feature_store.get('Hello, world!') == {'tokens': ['Hello', 'world'], 'pos': ['NNP', 'NN'], 'textstats': [1, 1]}\n",
    "\n",
    "text_feature_store.empty()\n",
    "text_feature_store.update('Hello, world!', ['Hello', 'world'], ['INTJ', 'NOUN'], [1, 1, 2, 1, 2, 1])\n",
    "assert text_feature_store.get_textstats_from_texts(['Hello, world!']) == [[1, 1, 2, 1, 2, 1]]\n",
    "assert text_feature_store.get_textstats_from_texts(['Hello, world!'], columns_out=['monosyll_count', 'polysyll_count']) == [[1, 1]]\n",
    "\n",
    "# this should raise a value error\n",
    "try:\n",
    "    textstats = text_feature_store.get_textstats_from_texts(['Hello, world!'], columns_out=['dr_dre'])\n",
    "except ValueError as e:\n",
    "    assert True\n",
    "else:\n",
    "    assert False, \"Expected ValueError not raised\"\n",
    "\n",
    "del text_feature_store\n",
    "os.remove('test_store.sqlite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
