{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexicons\n",
    "\n",
    "> Extract features from texts based on lexicons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from textplumber.store import TextFeatureStore\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LexiconCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "\t\"\"\" A Sci-kit Learn pipeline component to get document-level counts for one or more lexicons. \n",
    "\t\tThis component should be used after the SpacyPreprocessor component with the same feature store. \"\"\" \n",
    "\tdef __init__(self,\n",
    "\t\t\t  \t feature_store: TextFeatureStore, # the feature store to use - this should be the same feature store used in the SpacyPreprocessor component\n",
    "\t\t\t\t lexicons:dict, # the lexicons to use - a dictionary with the lexicon name as the key and the lexicon (a list of tokens to count) as the value\n",
    "\t\t\t\t lowercase:bool = True, # whether to lowercase the tokens\n",
    "\t\t\t\t ):\n",
    "\n",
    "\t\tself.feature_store = feature_store\n",
    "\n",
    "\t\tif not isinstance(lexicons, dict):\n",
    "\t\t\traise ValueError(\"Lexicons should be a dictionary with lexicon name as the key and the lexicon (a list of tokens to count) as the value.\")\n",
    "\n",
    "\t\tself.lexicons = lexicons\n",
    "\t\tself.lowercase = lowercase\n",
    "\t\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\t\"\"\" Fit the vectorizer to the tokens in the feature store. \"\"\"\n",
    "\t\treturn self\n",
    "\t\n",
    "\tdef transform(self, X):\n",
    "\t\t\"\"\" Transform the texts to a matrix of counts. \"\"\"\n",
    "\t\tdocs_tokens = self.feature_store.get_tokens_from_texts(X, lowercase = self.lowercase)\n",
    "\t\tX = []\n",
    "\t\tfor doc in docs_tokens:\n",
    "\t\t\tlexicon_counts = []\n",
    "\t\t\tfor lexicon in self.lexicons:\n",
    "\t\t\t\tlexicon_counts.append(sum([1 for token in doc if token in self.lexicons[lexicon]]))\n",
    "\t\t\tX.append(lexicon_counts)\n",
    "\t\treturn np.array(X)\n",
    "\t\t\n",
    "\tdef get_feature_names_out(self, input_features=None):\n",
    "\t\t\"\"\" Get the feature names out from the vectorizer. \"\"\"\n",
    "\t\treturn list(self.lexicons.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_empath_lexicons():\n",
    "    \"\"\" Get the empath lexicons from the empath github repo. \"\"\"\n",
    "    empath_lexicon = 'https://raw.githubusercontent.com/Ejhfast/empath-client/refs/heads/master/empath/data/categories.tsv'\n",
    "\n",
    "    empath_text = requests.get(empath_lexicon).text.strip()\n",
    "\n",
    "    empath_lexicons = {}\n",
    "    lines = empath_text.split('\\n')\n",
    "    for line in lines:\n",
    "        tokens = line.split()\n",
    "        tokens = [token for token in tokens if token != '']\n",
    "        if len(tokens) > 0:\n",
    "            # first token is name and a candidate token\n",
    "            empath_lexicons[tokens[0]] = tokens\n",
    "\n",
    "    return empath_lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from textplumber.preprocess import SpacyPreprocessor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "\n",
    "feature_store = TextFeatureStore('../test-data/test_lexicons')\n",
    "lexicon = {\n",
    "    'positive': ['good', 'happy', 'joyful'],\n",
    "    'negative': ['bad', 'sad', 'angry']\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('spacy_preprocessor', SpacyPreprocessor(feature_store=feature_store)),\n",
    "    ('lexicon_count_vectorizer', LexiconCountVectorizer(feature_store=feature_store, lexicons=lexicon))\n",
    "])\n",
    "docs = ['I am happy', 'I am sad', 'I am angry', 'I am happy happy']\n",
    "X = pipeline.fit_transform(docs)\n",
    "\n",
    "id = pipeline.named_steps['lexicon_count_vectorizer'].get_feature_names_out().index('positive')\n",
    "assert X.tolist()[0][id] == 1\n",
    "assert X.tolist()[0][id] == 1\n",
    "assert X.tolist()[3][id] == 2\n",
    "id = pipeline.named_steps['lexicon_count_vectorizer'].get_feature_names_out().index('negative')\n",
    "assert X.tolist()[0][id] == 0\n",
    "\n",
    "os.remove('../test-data/test_lexicons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
