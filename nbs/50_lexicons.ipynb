{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexicons\n",
    "\n",
    "> Extract features from texts based on lexicons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from textplumber.store import TextFeatureStore\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LexiconCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "\t\"\"\" A Sci-kit Learn pipeline component to get document-level counts for one or more lexicons. \n",
    "\t\tThis component should be used after the SpacyPreprocessor component with the same feature store. \"\"\" \n",
    "\tdef __init__(self,\n",
    "\t\t\t  \t feature_store: TextFeatureStore, # the feature store to use - this should be the same feature store used in the SpacyPreprocessor component\n",
    "\t\t\t\t lexicons:dict, # the lexicons to use - a dictionary with the lexicon name as the key and the lexicon (a list of tokens to count) as the value\n",
    "\t\t\t\t lowercase:bool = True, # whether to lowercase the tokens\n",
    "\t\t\t\t ):\n",
    "\n",
    "\t\tself.feature_store = feature_store\n",
    "\n",
    "\t\tif not isinstance(lexicons, dict):\n",
    "\t\t\traise ValueError(\"Lexicons should be a dictionary with lexicon name as the key and the lexicon (a list of tokens to count) as the value.\")\n",
    "\n",
    "\t\tself.lexicons = lexicons\n",
    "\t\tself.lowercase = lowercase\n",
    "\t\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\t\"\"\" Fit the vectorizer to the tokens in the feature store. \"\"\"\n",
    "\t\treturn self\n",
    "\t\n",
    "\tdef transform(self, X):\n",
    "\t\t\"\"\" Transform the texts to a matrix of counts. \"\"\"\n",
    "\t\tdocs_tokens = self.feature_store.get_tokens_from_texts(X, lowercase = self.lowercase)\n",
    "\t\tX = []\n",
    "\t\tfor doc in docs_tokens:\n",
    "\t\t\tlexicon_counts = []\n",
    "\t\t\tfor lexicon in self.lexicons:\n",
    "\t\t\t\tlexicon_counts.append(sum([1 for token in doc if token in self.lexicons[lexicon]]))\n",
    "\t\t\tX.append(lexicon_counts)\n",
    "\t\treturn np.array(X)\n",
    "\t\t\n",
    "\tdef get_feature_names_out(self, input_features=None):\n",
    "\t\t\"\"\" Get the feature names out from the vectorizer. \"\"\"\n",
    "\t\treturn list(self.lexicons.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geoff/miniconda3/envs/textplumber/lib/python3.11/site-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from textplumber.preprocess import SpacyPreprocessor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "feature_store = TextFeatureStore()\n",
    "spacy_preprocessor = SpacyPreprocessor(feature_store=feature_store)\n",
    "lexicon = {\n",
    "    'positive': ['good', 'happy', 'joyful'],\n",
    "    'negative': ['bad', 'sad', 'angry']\n",
    "}\n",
    "lexicon_count_vectorizer = LexiconCountVectorizer(feature_store=feature_store, lexicons=lexicon)\n",
    "pipeline = Pipeline([\n",
    "    ('spacy_preprocessor', spacy_preprocessor),\n",
    "    ('lexicon_count_vectorizer', lexicon_count_vectorizer)\n",
    "])\n",
    "docs = ['I am happy', 'I am sad', 'I am angry', 'I am happy happy']\n",
    "pipeline.fit(docs)\n",
    "X = pipeline.transform(docs)\n",
    "\n",
    "id = lexicon_count_vectorizer.get_feature_names_out().index('positive')\n",
    "assert X.tolist()[0][id] == 1\n",
    "assert X.tolist()[0][id] == 1\n",
    "assert X.tolist()[3][id] == 2\n",
    "id = lexicon_count_vectorizer.get_feature_names_out().index('negative')\n",
    "assert X.tolist()[0][id] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
