"""Extract features from texts based on lexicons."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/50_lexicons.ipynb.

# %% auto 0
__all__ = ['LexiconCountVectorizer', 'get_empath_lexicons']

# %% ../nbs/50_lexicons.ipynb 3
from sklearn.base import BaseEstimator, TransformerMixin
from .store import TextFeatureStore
import numpy as np
import requests

# %% ../nbs/50_lexicons.ipynb 4
class LexiconCountVectorizer(BaseEstimator, TransformerMixin):
	""" A Sci-kit Learn pipeline component to get document-level counts for one or more lexicons. 
		This component should be used after the SpacyPreprocessor component with the same feature store. """ 
	def __init__(self,
			  	 feature_store: TextFeatureStore, # the feature store to use - this should be the same feature store used in the SpacyPreprocessor component
				 lexicons:dict, # the lexicons to use - a dictionary with the lexicon name as the key and the lexicon (a list of tokens to count) as the value
				 lowercase:bool = True, # whether to lowercase the tokens
				 ):

		self.feature_store = feature_store

		if not isinstance(lexicons, dict):
			raise ValueError("Lexicons should be a dictionary with lexicon name as the key and the lexicon (a list of tokens to count) as the value.")

		self.lexicons = lexicons
		self.lowercase = lowercase
	
	def fit(self, X, y=None):
		""" Fit the vectorizer to the tokens in the feature store. """
		return self
	
	def transform(self, X):
		""" Transform the texts to a matrix of counts. """
		docs_tokens = self.feature_store.get_tokens_from_texts(X, lowercase = self.lowercase)
		X = []
		for doc in docs_tokens:
			lexicon_counts = []
			for lexicon in self.lexicons:
				lexicon_counts.append(sum([1 for token in doc if token in self.lexicons[lexicon]]))
			X.append(lexicon_counts)
		return np.array(X)
		
	def get_feature_names_out(self, input_features=None):
		""" Get the feature names out from the vectorizer. """
		return list(self.lexicons.keys())

# %% ../nbs/50_lexicons.ipynb 5
def get_empath_lexicons():
    """ Get the empath lexicons from the empath github repo. """
    empath_lexicon = 'https://raw.githubusercontent.com/Ejhfast/empath-client/refs/heads/master/empath/data/categories.tsv'

    empath_text = requests.get(empath_lexicon).text.strip()

    empath_lexicons = {}
    lines = empath_text.split('\n')
    for line in lines:
        tokens = line.split()
        tokens = [token for token in tokens if token != '']
        if len(tokens) > 0:
            # first token is name and a candidate token
            empath_lexicons[tokens[0]] = tokens

    return empath_lexicons
