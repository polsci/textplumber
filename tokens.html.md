# tokens


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

------------------------------------------------------------------------

<a
href="https://github.com/polsci/textplumber/blob/main/textplumber/tokens.py#L15"
target="_blank" style="float:right; font-size:smaller">source</a>

### TokensVectorizer

>  TokensVectorizer (feature_store:textplumber.store.TextFeatureStore,
>                        vectorizer_type:str='count', lowercase:bool=False,
>                        min_token_length:int=0, remove_punctuation:bool=False,
>                        remove_numbers:bool=False,
>                        stop_words:list[str]|None=None, min_df:float|int=1,
>                        max_df:float|int=1.0, max_features:int=5000,
>                        ngram_range:tuple=(1, 1), vocabulary:list|None=None,
>                        encoding:str='utf-8', decode_error:str='ignore')

*Sci-kit Learn pipeline component to extract token features. This
component should be used after the SpacyPreprocessor component with the
same feature store. The component gets the tokens from the feature store
and returns a matrix of counts (via CountVectorizer) or Tf-idf scores
(using TfidfVectorizer).*

<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 25%" />
<col style="width: 34%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>feature_store</td>
<td>TextFeatureStore</td>
<td></td>
<td>the feature store to use - this should be the same feature store
used in the SpacyPreprocessor component</td>
</tr>
<tr>
<td>vectorizer_type</td>
<td>str</td>
<td>count</td>
<td>the type of vectorizer to use - ‘count’ for CountVectorizer or
‘tfidf’ for TfidfVectorizer</td>
</tr>
<tr>
<td>lowercase</td>
<td>bool</td>
<td>False</td>
<td>whether to lowercase the tokens</td>
</tr>
<tr>
<td>min_token_length</td>
<td>int</td>
<td>0</td>
<td>the minimum token length to use</td>
</tr>
<tr>
<td>remove_punctuation</td>
<td>bool</td>
<td>False</td>
<td>whether to remove punctuation from the tokens</td>
</tr>
<tr>
<td>remove_numbers</td>
<td>bool</td>
<td>False</td>
<td>whether to remove numbers from the tokens</td>
</tr>
<tr>
<td>stop_words</td>
<td>list[str] | None</td>
<td>None</td>
<td>the stop words to use - passed to CountVectorizer or
TfidfVectorizer</td>
</tr>
<tr>
<td>min_df</td>
<td>float | int</td>
<td>1</td>
<td>the minimum document frequency to use - passed to CountVectorizer or
TfidfVectorizer</td>
</tr>
<tr>
<td>max_df</td>
<td>float | int</td>
<td>1.0</td>
<td>the maximum document frequency to use - passed to CountVectorizer or
TfidfVectorizer</td>
</tr>
<tr>
<td>max_features</td>
<td>int</td>
<td>5000</td>
<td>the maximum number of features to use, setting a default to avoid
memory issues - passed to CountVectorizer or TfidfVectorizer</td>
</tr>
<tr>
<td>ngram_range</td>
<td>tuple</td>
<td>(1, 1)</td>
<td>the ngram range to use (min_n, max_n) - passed to CountVectorizer or
TfidfVectorizer</td>
</tr>
<tr>
<td>vocabulary</td>
<td>list | None</td>
<td>None</td>
<td>list of tokens to use - passed to CountVectorizer or
TfidfVectorizer</td>
</tr>
<tr>
<td>encoding</td>
<td>str</td>
<td>utf-8</td>
<td>the encoding to use - passed to CountVectorizer or
TfidfVectorizer</td>
</tr>
<tr>
<td>decode_error</td>
<td>str</td>
<td>ignore</td>
<td>what to do if there is an error decoding ‘strict’, ‘ignore’,
‘replace’ - passed to CountVectorizer or TfidfVectorizer</td>
</tr>
</tbody>
</table>

TODO: add an example.
